---
layout: posts
title: Irreproducibility
---

<article>
    <header>
        <h2>Irreproducibility</h2>
        <time datetime="2018-03-10T11:50">Mar 10</time>
    </header>

    <img src="/img/irreproduce.jpg" alt="Magnifying glass and graphs">

    <p>
        I can sometimes find it difficult to even wrap my mouth around the word, but according to <a href="https://www.vocabulary.com/dictionary/irreproducibility" rel="noopener noreferrer" target="_blank">vocabulary.com</a>, 'irreproducibility' refers to 'the trait of not being dependable (n)or reliable.'
        In the case of research, it has come up for social sciences and also <a href="https://www.federalreserve.gov/econresdata/feds/2015/files/2015083pap.pdf" rel="noopener noreferrer" target="_blank">economics</a>.
        If you haven't heard about it, one of my favourite podcasts Planet Money gives a <a href="https://www.npr.org/sections/money/2016/01/15/463237871/episode-677-the-experiment-experiment" rel="noopener noreferrer" target="_blank">great overview</a> of the issue.
        You can also find a <a href="https://www.nature.com/news/over-half-of-psychology-studies-fail-reproducibility-test-1.18248" rel="noopener noreferrer" target="_blank">brief summary</a> of the issues in Nature,
        and <a href="http://science.sciencemag.org/content/349/6251/aac4716.full" rel="noopener noreferrer" target="_blank">full details</a> of the research in Science, and follow up research to <a href="https://www.nature.com/articles/s41562-016-0021" rel="noopener noreferrer" target="_blank">propose solutions to  the problem</a> in Nature.
    </p>
    <p>
        <a href="https://www.nature.com/news/over-half-of-psychology-studies-fail-reproducibility-test-1.18248" rel="noopener noreferrer" target="_blank"><img style="margin-left:1.0em; margin-bottom:0.2em" class="other-img" src="https://www.nature.com/polopoly_fs/7.25852.1440587393!/image/replicatation-graphic-b.png_gen/derivatives/landscape_300/replicatation-graphic-b.png" align="right" alt="nature-article-pic"></a>

        <h3>The Problem</h3>
        <ul>
            <li>Psychologist Brian Nosek initiated a project to examine the reproducibility of social psychology studies</li>
            <li>He gathered a team of researchers who attempted to reproduce the results of 100 studies published in reputible journals</li>
            <li>When the results were gathered, only 39 of the studies were considered to be reproduced</li>
        </ul>
        <p>While there is some variability in what was considered to be reproduced (as can be seen in the image to the right), these were still concerning findings and threatened the confidence placed in the scientific process for published research.</p>

        <h3>The Causes</h3>
        <p>Some of the potential causes for these issues, as suggested by Nosek and others, include:</p>
        <ul>
            <li>Major research journals do not typically publish studies that produce non-significant results</li>
            <li>Currently close to 0% of available research funding is directed towards reproducing the results of other studies</li>
            <li>Researchers can inadvertently change sample sizes or other seemingly innocent/appropriate modifications if they appear close to a significant result</li>
        </ul>

        <h3>The Solutions</h3>
        <p>Some of the solutions proposed by Nosek and others include:</p>
        <ul>
            <li>Dedicating 3% of research funding to reproducing studies</li>
            <li>Requiring researchers to indicate their intention to complete a study, with planned analysis methodologies, in an accessible repository</li>
            <li>Using this repository to confirm that researchers maintained their proposed methodology</li>
        </ul>
    </p>
    <h3>The Implications</h3>
    <p>
        But what does any of this have to do with the field of data science? At least to me, there are some quite clear connections.
        Essentially what Nosek et al. demonstrated is that current practices associated with reputible, peer-reviewed journals have appeared to increase the likelihood of Type I errors in published research.
        Given that the hypothesis testing that is involved in data analysis is often an interative process, based on the examination of data, and allowing the data to inform the questions asked, and review of these methodologies likely does not face the same scrutiny as those of peer-reviewed journals, it would seem that data science testing and modelling <strong>can be susceptible to the same issues</strong>.
        <br>
        <br>
        The question is, how is this being addressed? My truthful answer at this time is, <strong>I don't know</strong>!
        It is something that I am looking forward to asking more informed people than I am maybe I will be back in the future with edits if I get some answers. :)
    </p>
</article>
