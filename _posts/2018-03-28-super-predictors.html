---
layout: posts
title: Super Predictors
---

<article>
    <header>
        <h2>Super Predictors</h2>
        <time datetime="2018-03-28T16:35">Mar 28</time>
    </header>

    <img src="/img/machine.jpg" alt="Robot thinking">

    <p>
        Another <a href="https://www.npr.org/2017/06/26/534120962/degrees-of-maybe-how-we-can-all-make-better-predictions" rel="noopener noreferrer" target="_blank">podcast</a> contemplation today.
        This time from Hidden Brain, another of my favourite podcasts.
        The guest on the show was one of the co-authors of a book called "Superforecasting: The Art and Science of Prediction," Phil Tetlock.
        (Might have to add this to my reading list)
        Hmm, at least sounds related to data science, right?
    </p>
    <p>
        The story's actually quite fun. In this case, the US government decided to pit some of their expert analyst teams against others to compare powers of prediction on security related matters, such as the probability that Russia will invade "x" country in the next 18 months.
        What they found is that Tetlock's team (which apparently included a retired irrigation specialist and former ballroom dancer), ended up <strong>doing better in their predictions than the security experts</strong>, who had access to classified information that the lay teams did not have.
        How did they do it?
    </p>
    <p>
        Bringing in another favourite topic of mine, <a href="https://www.mindsetworks.com/science/" rel="noopener noreferrer" target="_blank">growth-mindset</a>, they approached these tasks from a persepctive that prediction was a skill that could be learned and improved upon over time.
        Starting with that premise, here's some of the approaches they employed.
    </p>
    <h3>1. Start from the outside-in</h3>
    <img class="right-img" src="/img/study.jpg" align="right" alt="Robot thinking">
    <p>

        Here's how most of us make predictions:
        <br>
        Let's say you're friend asks you whether you think they will fail their statistics final exam tomorrow.
        <br>
        Some of the things you are likely to consider are:
    </p>
    <ul type="square">
        <li>What you know about your friend's capabilities in statistics</li>
        <li>How much they've studied</li>
        <li>How much sleep they'll get</li>
        <li>Whether they typically do well in testing conditions</li>
        <li>Whether they have anything else going on that might impact their success</li>
    </ul>
    <p>
        But apparently this is to our detriment! It's not that any of these are terrible things to consider, it's just that <strong>starting</strong> here is not so great.
        <br>
        <br>
        Here's what people skilled at predition do instead:
        <br>
        Start with a baseline that draws from knowledge of others in similar circumstances.
        <br>
        <br>
        So, to answer your friend's question, what you should be doing is:
    </p>
    <ul type="square">
        <li>Consider the average passing rate for people who have taken the statistics final exam</li>
        <li>Discover if there are any personal characteristics shared by your friend that seem to contribute to passing the exam</li>
        <li><strong>Only</strong> after you've done all of that, add in some small adjustments for the specifics of your friend's situation (that were mentioned above)</li>
    </ul>
    <p>
        Which leads to the second suggestion they made.
    </p>
    <h3>2. Good prediction is degrees of 'maybe'</h3>
    <p>
        As stated in the show, when most lay people are making predictions they usually pick from one of three categories:
    </p>
    <ol>
        <li>Yes</li>
        <li>No</li>
        <li>Maybe</li>
    </ol>
    <p>
        But just as we were to make adjustments to our baseline estimate for our friend's exam success, effective predictors have much more precise measurements within 'maybe' of where the probability of an event lies.
        Being able to narrow in on the difference in a couple of percentage points for the probability of an event is critical.
    </p>
    <h3>3. Make regular adjustments</h3>
    <p>
        One of the things they said was that good prediction regularly udpates the model based on small changes.
        By themselves, these small changes might not seem significant, but especially compounded on each other over time, these can add up.
        <br>
        <br>
        So in the case of our friend, the next they come and ask us whether we think they will fail their NEXT final, we shouldn't just use the same model that we used for the previous statistics exam.
        Instead, we should be adjusting our model to incorporate the changes in conditions for this situation.
    </p>
    <h3>My takeaways</h3>
    <p>
        This really does remind me of some of the conversations that we do have in social services.
        Findings that are made, and the decisions based on them, are often so static.
        We complete research, we make findings, we make decisions for action based on these findings.
        But it is far less common for us to update our research, or test the previous assumptions based on new findings.
        (I think one of the reasons for this actually has to do with the nature of the type of research that my field of social sciences does, but that might get us too far off track)
        <br>
        <br>
        <strong>So my biggest takeaway:</strong> For findings or models to create accurate predictions for the future, they need to be regularly updated.
        <br>
        <br>
        Though, hearing all of this today, does give me some hope.
        If we can make predictions about whether Russia WILL invade some country in 18months within some level of probability, surely that means we can do things like model the workings of homelessness in a city to better determine what types of interventions could help people exit homelessness, right?
    </p>
</article>
